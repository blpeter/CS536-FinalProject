{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379e7857-9809-4754-809a-7ebd19fef4d2",
   "metadata": {},
   "source": [
    "### Load MSCOCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0756beb2-2d50-4fe8-abd2-752e7f4861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refer import REFER\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image as PImage # pillow\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7abaabb-2dc4-42d9-bcc1-4fc5d79920d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset refcoco into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=5.22s)\n"
     ]
    }
   ],
   "source": [
    "data_root = 'coco'  # contains refclef, refcoco, refcoco+, refcocog and images\n",
    "dataset = 'refcoco' \n",
    "splitBy = 'unc'\n",
    "refer = REFER(data_root, dataset, splitBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f011a19-a156-4fa2-8a69-f2f15a17ee9e",
   "metadata": {},
   "source": [
    "### Load the SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a226de04-1e79-4ab4-b13f-dc7776eb7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itadmin/anaconda3/envs/A8/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d698bd-fbf4-4d46-abe6-245fc8b94fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a106630-a7ba-407c-a0b2-34dac3e7f2ff",
   "metadata": {},
   "source": [
    "### Prepare the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1cc1a2-6f4c-451f-b572-21a7ef1f4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prep(img_id, ann_id):\n",
    "    \n",
    "    #get image and bounding box\n",
    "    img = refer.Imgs[img_id]\n",
    "    bb = refer.Anns[ann_id]['bbox']\n",
    "    fname = os.path.join(refer.IMAGE_DIR, img['file_name'])\n",
    "    \n",
    "    #load image \n",
    "    image = cv2.imread(fname)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #load image into SAM\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    #find midpoint and boundign box for SAM\n",
    "    bbox = refer.Anns[ann_id]['bbox']\n",
    "    bbox = [int(b) for b in bbox]\n",
    "    x = (bbox[0] + (bbox[2]/2))\n",
    "    y = (bbox[1] + (bbox[3]/2))\n",
    "    input_point = np.array([[x, y]])\n",
    "    input_label = np.array([1])\n",
    "    input_box = np.array([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]) #xyxy format\n",
    "    \n",
    "    #get and apply mask\n",
    "    masks, scores, _ = predictor.predict(point_coords=input_point, point_labels=input_label,  box=input_box, multimask_output=True)\n",
    "    index_max = np.argmax(scores)\n",
    "    image[~masks[index_max],:] = [255,255,255]\n",
    "\n",
    "    s_image = image[bbox[1]:bbox[1] + bbox[3], bbox[0]:bbox[0] + bbox[2]]\n",
    "    \n",
    "    #normalize image for processing\n",
    "    xs = 224\n",
    "    ys = 224\n",
    "    if len(s_image) == 0: return None\n",
    "    pim = PImage.fromarray(s_image)\n",
    "    pim2 = pim.resize((xs,ys), PImage.Resampling.LANCZOS)\n",
    "    img = np.array(pim2)\n",
    "    \n",
    "    if len(img.shape) < 3: return None\n",
    "    \n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    \n",
    "    return pim  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ee1c1-db56-490e-9cf1-fbc57cf81bcf",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3099c1fa-fec9-4a58-a027-933c023191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "#device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "from collections import defaultdict as dd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc1c1a1-2d13-4b17-b3e3-f80a5429b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posfeats(img_id, ann_id,):\n",
    "    img = refer.Imgs[img_id]\n",
    "    bb = refer.Anns[ann_id]['bbox']\n",
    "    fname = os.path.join(refer.IMAGE_DIR, img['file_name'])\n",
    "    if not os.path.isfile(fname): return None\n",
    "    img = io.imread(fname)\n",
    "    \n",
    "    if len(img.shape) < 3: return None\n",
    "    ih, iw, _ = img.shape\n",
    "    x,y,w,h = bb\n",
    "    # x1, relative\n",
    "    x1r = x / iw\n",
    "    # y1, relative\n",
    "    y1r = y / ih\n",
    "    # x2, relative\n",
    "    x2r = (x+w) / iw\n",
    "    # y2, relative\n",
    "    y2r = (y+h) / ih\n",
    "    # area\n",
    "    area = (w*h) / (iw*ih)\n",
    "    # ratio image sides (= orientation)\n",
    "    ratio = iw / ih\n",
    "    # distance from center (normalised)\n",
    "    cx = iw / 2\n",
    "    cy = ih / 2\n",
    "    bcx = x + w / 2\n",
    "    bcy = y + h / 2\n",
    "    distance = np.sqrt((bcx-cx)**2 + (bcy-cy)**2) / np.sqrt(cx**2+cy**2)\n",
    "    # done!\n",
    "    return np.array([x1r,y1r,x2r,y2r,area,ratio,distance]).reshape(1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c1d68b-7d6b-45fb-92de-4f77dcb39550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91005/3234115306.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(train_ids):#[:1000]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13b3851f36f48819abbe0e159d6523c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_as_classifiers = dd(list) # use something like this dictionary to store positive examples\n",
    "train_ids = refer.getRefIds(split='train')\n",
    "\n",
    "for i in tqdm(train_ids):#[:1000]):\n",
    "    # first, get all of the training dat\n",
    "    ref = refer.Refs[i]\n",
    "    \n",
    "    # for a single train_id, you can get its image_id and the ann_id (i.e., the referring expression)\n",
    "    img_id = ref['image_id']\n",
    "    ann_id = ref['ann_id']\n",
    "\n",
    "    #prepare image to pass to clip\n",
    "    if (image_prep(img_id, ann_id) and compute_posfeats(img_id, ann_id) is not None):\n",
    "        img = image_prep(img_id, ann_id)\n",
    "    \n",
    "        #then, you'll need to pass that image through a convnet like you did for A6\n",
    "        img = preprocess(img).unsqueeze(0).to(device)\n",
    "        enc_img = clip_model.encode_image(img)\n",
    "    \n",
    "        #optionally, you can call the compute_posfeats function to get some additional features\n",
    "        #concatenate these to the convnet output to form a single vector for this image\n",
    "        pos_feats = compute_posfeats(img_id, ann_id)\n",
    "        feature_vector = np.concatenate(( enc_img.detach().cpu().numpy(), pos_feats), axis=1)\n",
    "\n",
    "        #add this feature vector to a list of positive examples for each word in the referring expression\n",
    "        # you may need to flatten() the feature vector\n",
    "        for sent in ref['sentences']:\n",
    "            for word in sent['tokens']:\n",
    "                words_as_classifiers[word].append(feature_vector)\n",
    "\n",
    "\n",
    "#for word in words_as_classifiers:\n",
    "#   print(word, len(words_as_classifiers[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a832eec5-b9c6-456f-b043-15ef5347cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_negative_samples(words_as_classifiers, word):\n",
    "    words = list(words_as_classifiers.keys())\n",
    "    words.remove(word)\n",
    "    random_word = random.choice(words)\n",
    "    random_vector = random.choice(words_as_classifiers[random_word])\n",
    "    return random_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5515c03e-dfe3-4da7-a999-d0c692754842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91005/435344039.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for word in tqdm(words_as_classifiers):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6423a20113604c27b5452a0bc5cb1182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now that we have all of the positive examples for all of the words, we  need to find negative examples for each word\n",
    "\n",
    "num_negatives = 2 \n",
    "threshold = 4\n",
    "\n",
    "wac = {}\n",
    "\n",
    "for word in tqdm(words_as_classifiers):\n",
    "    pos_vectors = words_as_classifiers[word]\n",
    "    num_pos_vectors = len(pos_vectors)\n",
    "    if num_pos_vectors < threshold:\n",
    "        continue\n",
    "#     print(word, num_pos_vectors)\n",
    "    neg_vectors = []\n",
    "    # the number of negative examples should be a function of how many positive examples there are\n",
    "    for i in range(0,num_negatives*num_pos_vectors):\n",
    "        neg_vectors.append(find_negative_samples(words_as_classifiers,word))\n",
    "    neg_vectors = np.array(neg_vectors)\n",
    "    pos_vectors = np.array(pos_vectors)\n",
    "    neg_vectors = neg_vectors.reshape(neg_vectors.shape[0], neg_vectors.shape[2])\n",
    "    pos_vectors = pos_vectors.reshape(pos_vectors.shape[0], pos_vectors.shape[2])\n",
    "#     print(pos_vectors.shape, neg_vectors.shape)\n",
    "    X = np.concatenate((pos_vectors, neg_vectors), axis=0)\n",
    "    \n",
    "    y = np.concatenate((np.ones(num_pos_vectors), np.zeros(len(neg_vectors))))\n",
    "    wac[word] = (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afd2862d-c221-4683-b7f3-2f6b03c4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "277825c1-a8fb-44be-921c-ca4061d600e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wac_si_Model.pickle', 'wb') as f:\n",
    "    pickle.dump(wac, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c904136a-8cd1-4a53-b01f-5a1543f0c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wac_si_Model.pickle', 'rb') as f:\n",
    "    wac = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10b00993-69cd-475e-863d-6fd1f841c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50056e25-a92a-43b5-8035-34339419ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, train a binary classifier for each word\n",
    "for word in wac:\n",
    "    clfr = LogisticRegression(C=0.25, max_iter=1000)\n",
    "    X,y = wac[word]\n",
    "    clfr.fit(X,y)\n",
    "    wac[word] = clfr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ca70c-e3bf-46d7-8790-5ca14e455362",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02af30f4-abe4-4925-a46c-7c8ef23756f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ids = refer.getRefIds(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e5b42e1-abba-4280-9532-74a7a27c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subimage(bbox, img, img_id, ann_id, xs=224,ys=224): \n",
    "\n",
    "    img= image_prep(img_id, ann_id)\n",
    "    if img is None: return None\n",
    "    img = preprocess(img).unsqueeze(0).to(device)\n",
    "    enc_img = clip_model.encode_image(img)\n",
    "    \n",
    "    pos_feats = compute_posfeats(img_id, ann_id)\n",
    "    if pos_feats is None: return None\n",
    "        \n",
    "    feature_vector = np.concatenate(( enc_img.detach().cpu().numpy(), pos_feats), axis=1)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "169fc2f9-59ae-4dc8-850e-92fc51e6361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    score = 0\n",
    "    total = 0 \n",
    "    # step through the eval ids\n",
    "    for i in tqdm(eval_ids):#[:10]):\n",
    "   \n",
    "        ref_id = i\n",
    "        ref = refer.Refs[ref_id]\n",
    "        #this is the gold annotation id for all of the sentences\n",
    "        ann_id = ref['ann_id']  \n",
    "        img_id = ref['image_id']\n",
    "        img = refer.Imgs[img_id]\n",
    "        # objs is a list of all of the object annotations for the image, including the gold\n",
    "        objs = refer.imgToAnns[img_id] \n",
    "    \n",
    "   \n",
    "        features = {}\n",
    "        for obj in objs:\n",
    "            # object as feature vector\n",
    "            features[obj['id']] = process_subimage(obj['bbox'], img, img_id, obj['id'])\n",
    "\n",
    "        # apply all of the feature vectors to your trained classifiers for each word in the sentence\n",
    "        for sent in ref['sentences']:\n",
    "            total += 1\n",
    "            pval = {oid: 1 for oid in features}\n",
    "            for oid in features:\n",
    "                feature = features[oid]\n",
    "                if feature is not None:\n",
    "                    for word in sent['tokens']: \n",
    "                        if (word in wac):\n",
    "                            # multiply the classifier probabilities together for each word\n",
    "                            pval[oid] *= wac[word].predict_proba(feature)[0][1]\n",
    "                else:\n",
    "                  pval[oid] = 0\n",
    "         \n",
    "            # find the object with the highest resulting multiplied probability, compare to gold \n",
    "            most_probable = max(pval, key=pval.get)\n",
    "            if (most_probable == ann_id):\n",
    "                score += 1   \n",
    "    \n",
    "    #return accuracy\n",
    "    return score/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e70535b8-bd54-4d7b-bb84-74e69285b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91005/3726591716.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(eval_ids):#[:10]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f412b44d1514eec9a605163951365a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate_score = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dac6636-0a58-4f2e-a64b-22cf8207021d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7029494382022472"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51a94a65-3c67-4403-bb0f-e0c3e69c5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ids = refer.getRefIds(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52ef70cf-bd71-4cbf-8ff9-ca77f09b21bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91005/3726591716.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(eval_ids):#[:10]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116794a2d0df415e847d86c9c810fb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_score = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be47a9ac-2706-4d61-9e54-4caa4e5a3937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6797805059523809"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153d92b-22c2-4a32-af26-2b6c677b10f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
