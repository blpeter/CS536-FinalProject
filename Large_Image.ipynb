{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "379e7857-9809-4754-809a-7ebd19fef4d2",
   "metadata": {},
   "source": [
    "### Load MSCOCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0756beb2-2d50-4fe8-abd2-752e7f4861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from refer import REFER\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image as PImage # pillow\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7abaabb-2dc4-42d9-bcc1-4fc5d79920d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset refcoco into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=5.40s)\n"
     ]
    }
   ],
   "source": [
    "data_root = 'coco'  # contains refclef, refcoco, refcoco+, refcocog and images\n",
    "dataset = 'refcoco' \n",
    "splitBy = 'unc'\n",
    "refer = REFER(data_root, dataset, splitBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f011a19-a156-4fa2-8a69-f2f15a17ee9e",
   "metadata": {},
   "source": [
    "### Load the SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a226de04-1e79-4ab4-b13f-dc7776eb7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d698bd-fbf4-4d46-abe6-245fc8b94fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a106630-a7ba-407c-a0b2-34dac3e7f2ff",
   "metadata": {},
   "source": [
    "### Prepare the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f73629-2098-462c-b423-2606d0200b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prep(img_id, ann_id):\n",
    "    \n",
    "    #get image and bounding box\n",
    "    img = refer.Imgs[img_id]\n",
    "    bb = refer.Anns[ann_id]['bbox']\n",
    "    fname = os.path.join(refer.IMAGE_DIR, img['file_name'])\n",
    "    \n",
    "    #load image \n",
    "    image = cv2.imread(fname)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #load image into SAM\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    #find midpoint and create points to load into SAM\n",
    "    bbox = refer.Anns[ann_id]['bbox']\n",
    "    bbox = [int(b) for b in bbox]\n",
    "    x = (bbox[0] + (bbox[2]/2))\n",
    "    y = (bbox[1] + (bbox[3]/2))\n",
    "    input_point = np.array([[x, y]])\n",
    "    input_label = np.array([1])\n",
    "    #bbox: [x_min, y_min, width, height]\n",
    "    input_box = np.array([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]) #xyxy format\n",
    "    \n",
    "    #get and apply mask\n",
    "    masks, scores, _ = predictor.predict(point_coords=input_point, point_labels=input_label,  box=input_box, multimask_output=True)\n",
    "    index_max = np.argmax(scores)\n",
    "    image[~masks[index_max],:] = [255,255,255]\n",
    "    \n",
    "    #Display masks \n",
    "    #for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    #    plt.figure(figsize=(10,10))\n",
    "    #    plt.imshow(image)\n",
    "    #    show_mask(mask, plt.gca())\n",
    "    #    show_box(input_box, plt.gca())\n",
    "    #    show_points(input_point, input_label, plt.gca())\n",
    "    #    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    #    plt.axis('off')\n",
    "    #    plt.show()\n",
    "        \n",
    "    #normalize image for processing\n",
    "    xs = 224\n",
    "    ys = 224\n",
    "    if len(image) == 0: return None\n",
    "    pim = PImage.fromarray(image)\n",
    "    pim2 = pim.resize((xs,ys), PImage.Resampling.LANCZOS)\n",
    "    img = np.array(pim2)\n",
    "    \n",
    "    if len(img.shape) < 3: return None\n",
    "    \n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    \n",
    "    return pim, img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ee1c1-db56-490e-9cf1-fbc57cf81bcf",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3099c1fa-fec9-4a58-a027-933c023191d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "#device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "from collections import defaultdict as dd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c1d68b-7d6b-45fb-92de-4f77dcb39550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38314/2466766897.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(train_ids):#[:1000]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e1d02e4a84de6ac58060aab12e75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_as_classifiers = dd(list) # use something like this dictionary to store positive examples\n",
    "train_ids = refer.getRefIds(split='train')\n",
    "\n",
    "for i in tqdm(train_ids):#[:1000]):\n",
    "    # first, get all of the training dat\n",
    "    ref = refer.Refs[i]\n",
    "    \n",
    "    # for a single train_id, you can get its image_id and the ann_id (i.e., the referring expression)\n",
    "    img_id = ref['image_id']\n",
    "    ann_id = ref['ann_id']\n",
    "    \n",
    "    img, _ = image_prep(img_id, ann_id)\n",
    "    #prepare image to pass to clip\n",
    "    if (img is not None):\n",
    "        #img, _ = image_prep(img_id, ann_id)\n",
    "    \n",
    "        #then, you'll need to pass that image through a convnet like you did for A6\n",
    "        img = preprocess(img).unsqueeze(0).to(device)\n",
    "        enc_img = clip_model.encode_image(img)\n",
    "    \n",
    "        #optionally, you can call the compute_posfeats function to get some additional features\n",
    "        #concatenate these to the convnet output to form a single vector for this image\n",
    "        #pos_feats = compute_posfeats(img_id, ann_id)\n",
    "        feature_vector = enc_img.detach().cpu().numpy()\n",
    "\n",
    "        #add this feature vector to a list of positive examples for each word in the referring expression\n",
    "        # you may need to flatten() the feature vector\n",
    "        for sent in ref['sentences']:\n",
    "            for word in sent['tokens']:\n",
    "                words_as_classifiers[word].append(feature_vector)\n",
    "\n",
    "\n",
    "#for word in words_as_classifiers:\n",
    "#   print(word, len(words_as_classifiers[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a832eec5-b9c6-456f-b043-15ef5347cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_negative_samples(words_as_classifiers, word):\n",
    "    words = list(words_as_classifiers.keys())\n",
    "    words.remove(word)\n",
    "    random_word = random.choice(words)\n",
    "    random_vector = random.choice(words_as_classifiers[random_word])\n",
    "    return random_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5515c03e-dfe3-4da7-a999-d0c692754842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38314/435344039.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for word in tqdm(words_as_classifiers):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96007370f314b84b522efbfce775b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now that we have all of the positive examples for all of the words, we  need to find negative examples for each word\n",
    "\n",
    "num_negatives = 2 \n",
    "threshold = 4\n",
    "\n",
    "wac = {}\n",
    "\n",
    "for word in tqdm(words_as_classifiers):\n",
    "    pos_vectors = words_as_classifiers[word]\n",
    "    num_pos_vectors = len(pos_vectors)\n",
    "    if num_pos_vectors < threshold:\n",
    "        continue\n",
    "#     print(word, num_pos_vectors)\n",
    "    neg_vectors = []\n",
    "    # the number of negative examples should be a function of how many positive examples there are\n",
    "    for i in range(0,num_negatives*num_pos_vectors):\n",
    "        neg_vectors.append(find_negative_samples(words_as_classifiers,word))\n",
    "    neg_vectors = np.array(neg_vectors)\n",
    "    pos_vectors = np.array(pos_vectors)\n",
    "    neg_vectors = neg_vectors.reshape(neg_vectors.shape[0], neg_vectors.shape[2])\n",
    "    pos_vectors = pos_vectors.reshape(pos_vectors.shape[0], pos_vectors.shape[2])\n",
    "#     print(pos_vectors.shape, neg_vectors.shape)\n",
    "    X = np.concatenate((pos_vectors, neg_vectors), axis=0)\n",
    "    \n",
    "    y = np.concatenate((np.ones(num_pos_vectors), np.zeros(len(neg_vectors))))\n",
    "    wac[word] = (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd2862d-c221-4683-b7f3-2f6b03c4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "277825c1-a8fb-44be-921c-ca4061d600e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wac_li_Model.pickle', 'wb') as f:\n",
    "    pickle.dump(wac, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c904136a-8cd1-4a53-b01f-5a1543f0c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wac_li_Model.pickle', 'rb') as f:\n",
    "    wac = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10b00993-69cd-475e-863d-6fd1f841c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50056e25-a92a-43b5-8035-34339419ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, train a binary classifier for each word\n",
    "for word in wac:\n",
    "    clfr = LogisticRegression(C=0.25, max_iter=1000)\n",
    "    X,y = wac[word]\n",
    "    clfr.fit(X,y)\n",
    "    wac[word] = clfr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa985f-c818-4d7a-96fa-a36999501fd9",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cadf27fb-af05-40d4-b7eb-b20c4f1f45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ids = refer.getRefIds(split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56b1a934-0084-486d-9fd2-9105a9f3db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subimage(bbox, img, img_id, ann_id, xs=224,ys=224): \n",
    "    img, _ = image_prep(img_id, ann_id)\n",
    "    img = preprocess(img).unsqueeze(0).to(device)\n",
    "    enc_img = clip_model.encode_image(img)\n",
    "    feature_vector = enc_img.detach().cpu().numpy()\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b4f542-332e-40cb-82b1-7cf560e523a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    score = 0\n",
    "    total = 0 \n",
    "    # step through the eval ids\n",
    "    for i in tqdm(eval_ids):#[:10]):\n",
    "   \n",
    "        ref_id = i\n",
    "        ref = refer.Refs[ref_id]\n",
    "        #this is the gold annotation id for all of the sentences\n",
    "        ann_id = ref['ann_id']  \n",
    "        img_id = ref['image_id']\n",
    "        img = refer.Imgs[img_id]\n",
    "        # objs is a list of all of the object annotations for the image, including the gold\n",
    "        objs = refer.imgToAnns[img_id] \n",
    "    \n",
    "   \n",
    "        features = {}\n",
    "        for obj in objs:\n",
    "            # object as feature vector\n",
    "            features[obj['id']] = process_subimage(obj['bbox'], img, img_id, obj['id'])\n",
    "\n",
    "        # apply all of the feature vectors to your trained classifiers for each word in the sentence\n",
    "        for sent in ref['sentences']:\n",
    "            total += 1\n",
    "            pval = {oid: 1 for oid in features}\n",
    "            for oid in features:\n",
    "                feature = features[oid]\n",
    "                if feature is not None:\n",
    "                    for word in sent['tokens']: \n",
    "                        if (word in wac):\n",
    "                            # multiply the classifier probabilities together for each word\n",
    "                            pval[oid] *= wac[word].predict_proba(feature)[0][1]\n",
    "                else:\n",
    "                  pval[oid] = 0\n",
    "         \n",
    "            # find the object with the highest resulting multiplied probability, compare to gold \n",
    "            most_probable = max(pval, key=pval.get)\n",
    "            if (most_probable == ann_id):\n",
    "                score += 1   \n",
    "    \n",
    "    #return accuracy\n",
    "    return score/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3570900-d07e-4d62-a1db-4572b35fae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38314/3726591716.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(eval_ids):#[:10]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17a07c29ff34dc7a82f62099278cdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate_score = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489bdfee-e495-4934-afed-11bf93bf0648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5771644821857117"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1a75b6-8dd9-4cce-83db-d0f0c280b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ids = refer.getRefIds(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a36386e-b591-489c-849d-256693e3a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38314/3726591716.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(eval_ids):#[:10]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88933f27efa04f3cb39621d14a16eadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_score = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0ac25dd-4eb0-4054-bc9f-05cedd705c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5825892857142857"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af76f7-e776-405f-a70d-27dc4d7c23bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
